# ماڈیول 5: کیپ اسٹون پروجیکٹ: اینڈ-ٹو-اینڈ ہیومینائیڈ روبوٹکس

## اینڈ-ٹو-اینڈ سسٹم انٹیگریشن: آواز سے عمل تک

کیپ اسٹون پروجیکٹ "فزیکل اے آئی اور ہیومینائیڈ روبوٹکس" ماڈیولز کے دوران حاصل کردہ تمام علم اور مہارتوں کو یکجا کرنے کا حتمی چیلنج ہے۔ اس کا مقصد ایک مکمل اینڈ-ٹو-اینڈ ہیومینائیڈ روبوٹکس سسٹم ڈیزائن، عمل درآمد، اور مظاہرہ کرنا ہے جو اعلیٰ سطحی قدرتی زبان کے احکامات کو سمجھنے، اپنے ماحول کو محسوس کرنے، پیچیدہ اعمال کی منصوبہ بندی کرنے، خود مختار طور پر نیویگیٹ کرنے، اور جسمانی ہیرا پھیری کرنے کے قابل ہو۔ یہ ماڈیول آپ کو مختلف اجزاء کو جوڑنے کے عمل کے ذریعے رہنمائی کرے گا—آواز کی شناخت اور بڑے زبان کے ماڈلز سے لے کر ROS 2 نیویگیشن اور جسمانی ہیرا پھیری تک—ایک مربوط، ذہین ایجنٹ میں۔

سسٹم کی پائپ لائن وائس-لینگویج-ایکشن (VLA) پیراڈائم کی قریب سے پیروی کرے گی، جہاں انسانی آواز کی ان پٹ کو قابل عمل روبوٹ رویوں میں تبدیل کیا جاتا ہے۔ اس کے لیے نہ صرف مضبوط انفرادی اجزاء کی ضرورت ہے بلکہ ان کے درمیان ہموار مواصلت اور ہم آہنگی کی بھی ضرورت ہے، جو اکثر ROS 2 فریم ورک کے اندر ترتیب دی جاتی ہے۔

## آواز سے متن: انسانی کمانڈ کو ڈیجیٹل ان پٹ میں پل کرنا

تفاعل کا سفر انسانی تقریر سے شروع ہوتا ہے۔ کیپ اسٹون میں ابتدائی کام ایک قابل اعتماد آواز سے متن پائپ لائن قائم کرنا ہے، جو ہیومینائیڈ روبوٹ کو بولے گئے احکامات کو درست طریقے سے ڈیجیٹل متن میں نقل کرنے کے قابل بناتا ہے۔ یہ متن پھر اعلیٰ سطحی علمی عملوں کے لیے بنیادی ان پٹ بن جاتا ہے۔

### کلیدی اجزاء:

1.  **مائیکروفون ارے:** اعلیٰ معیار کے مائیکروفونز واضح آڈیو حاصل کرنے کے لیے ضروری ہیں، خاص طور پر ممکنہ طور پر شور والے ماحول میں۔ ہیومینائیڈ روبوٹس کے لیے، ایک ارے سمت کی آگاہی فراہم کر سکتا ہے۔
2.  **اسپیچ-ٹو-ٹیکسٹ انجن:**
    *   **ماڈل:** جدید ماڈلز جیسے OpenAI's Whisper (یا اسی طرح کے اوپن سورس متبادل جیسے VOSK یا NVIDIA Riva) کا فائدہ اٹھانا۔
    *   **تعیناتی:** ماڈل کو یا تو روبوٹ کے ایمبیڈڈ سسٹم (مثلاً، NVIDIA Jetson برائے ایج انفرنسنگ) پر براہ راست چلایا جا سکتا ہے یا کلاؤڈ بیسڈ API پر آف لوڈ کیا جا سکتا ہے، تاخیر کی ضروریات اور کمپیوٹیشنل بجٹ کو مدنظر رکھتے ہوئے۔
    *   **مضبوطی:** درستگی کو بہتر بنانے کے لیے شور میں کمی، بازگشت کی منسوخی، اور آواز کی سرگرمی کا پتہ لگانا نافذ کریں۔
3.  **ROS 2 انٹرفیس:** ایک مخصوص ROS 2 نوڈ (`speech_to_text_node`) آڈیو کیپچر اور نقل کی انتظام کرے گا۔
    *   یہ خام آڈیو موضوعات کو سبسکرائب کرے گا یا آڈیو ہارڈ ویئر کے ساتھ براہ راست انٹرفیس کرے گا۔
    *   یہ نقل شدہ متن کو ایک نامزد ROS 2 موضوع (مثلاً، `/voice_commands/text`) پر شائع کرے گا، عام طور پر `std_msgs/String` پیغام کے طور پر۔

**ہارڈ ویئر کی ضروریات:** ایک موزوں مائیکروفون ارے اور ایک کمپیوٹنگ پلیٹ فارم جو منتخب کردہ اسپیچ-ٹو-ٹیکسٹ ماڈل کو مؤثر طریقے سے چلانے کے قابل ہو (مثلاً، NVIDIA Jetson Nano/Xavier NX برائے آن-روبوٹ پروسیسنگ)۔

## منصوبہ بندی: LLM بطور علمی مرکز

متن کمانڈ کے ساتھ، اگلا اہم قدم روبوٹ کے لیے انسانی ارادے کو سمجھنا اور ایک اسٹریٹجک منصوبہ بنانا ہے۔ یہ وہ جگہ ہے جہاں ایک بڑا زبان ماڈل (LLM) علمی مرکز کے طور پر کام کرتا ہے، جو تجریدی لسانی اہداف کو کنکریٹ، روبوٹ کے قابل عمل اعمال میں ترجمہ کرتا ہے۔

### LLM پر مبنی ٹاسک پلاننگ:

1.  **پرومپٹ انجینئرنگ:** LLM کو ایک احتیاط سے تیار کردہ پرومپٹ موصول ہوتا ہے جس میں شامل ہیں:
    *   نقل شدہ انسانی کمانڈ۔
    *   روبوٹ کی موجودہ حالت (مثلاً، مقام، بیٹری، موجودہ جوائنٹ پوزیشنز)۔
    *   ماحول کی وضاحت (مثلاً، اشیاء کا پتہ لگانا، نقشہ کا ڈیٹا)۔
    *   دستیاب روبوٹ "ٹولز" یا APIs کی فہرست (مثلاً، `navigate_to(location)`, `grasp(object_id)`, `speak(text)`)، ان کے پیرامیٹرز اور متوقع نتائج سمیت۔
    *   رکاوٹیں اور حفاظتی رہنما اصول (مثلاً، "ٹکراؤ سے بچیں"، "ممنوعہ علاقوں میں داخل نہ ہوں")۔
    *   کامیاب کمانڈ-ٹو-ایکشن سیکوینسز کی چند مثالیں۔
2.  **ٹاسک ڈیکمپوزیشن:** LLM اس معلومات کو پروسیس کرتا ہے اور ایک اعلیٰ سطحی منصوبہ تیار کرتا ہے، جو مرکزی مقصد کو ذیلی کاموں کی ترتیب میں توڑ دیتا ہے۔
3.  **ایکشن جنریشن:** ہر ذیلی کام کے لیے، LLM مناسب روبوٹ ٹولز کا انتخاب کرتا ہے اور مخصوص فنکشن کالز یا ROS 2 ایکشن گولز، مکمل پیرامیٹرز کے ساتھ تیار کرتا ہے۔
4.  **ROS 2 انٹرفیس (`llm_planner_node`):** ایک ROS 2 نوڈ:
    *   `/voice_commands/text` اور سینسر ڈیٹا موضوعات کو سبسکرائب کرے گا۔
    *   LLM کو پرومپٹس بھیجے گا (مقامی یا کلاؤڈ بیسڈ)۔
    *   LLM کے آؤٹ پٹ کو ROS 2 ایکشن گولز یا سروس کالز کی ترتیب میں پارس کرے گا۔
    *   ان گولز/کالز کو مناسب ROS 2 موضوعات/ایکشنز پر شائع کرے گا۔

**ہارڈ ویئر کی ضروریات:** ایک مضبوط انٹرنیٹ کنکشن (کلاؤڈ LLMs کے لیے) یا اہم مقامی کمپیوٹ (ایج LLMs کے لیے، مثلاً، NVIDIA Jetson Orin بڑی میموری کے ساتھ)۔

## نیویگیشن: جسمانی دنیا میں سفر کرنا

خود مختار حرکت بنیادی ہے۔ کیپ اسٹون پروجیکٹ کے لیے ایک نفیس نیویگیشن اسٹیک کو نافذ کرنے کی ضرورت ہے جو ہیومینائیڈ روبوٹ کو اپنے ماحول میں محفوظ اور مؤثر طریقے سے منتقل کرنے کی اجازت دیتا ہے، رکاوٹوں سے بچتے ہوئے اور مخصوص اہداف تک پہنچتے ہوئے۔

### کلیدی نیویگیشن اجزاء:

1.  **لوکلائزیشن اور میپنگ (VSLAM):**
    *   **سینسرز:** LiDAR، ڈیپتھ کیمرے (مثلاً، Intel RealSense)، اور IMUs کا انضمام۔
    *   **الگورتھم:** GPU-تیز رفتار VSLAM (مثلاً، Isaac ROS VSLAM) کا استعمال کرتے ہوئے مسلسل روبوٹ کی پوزیشن کا اندازہ لگانا اور ماحول کا 3D نقشہ بنانا/اپ ڈیٹ کرنا۔
    *   **ROS 2 انٹرفیس:** ایک `vslam_node` روبوٹ کی پوزیشن اور نقشہ کی اپ ڈیٹس شائع کرتا ہے۔
2.  **پاتھ پلاننگ:**
    *   **گلوبل پلانر:** روبوٹ کی موجودہ جگہ سے منزل تک ایک اعلیٰ سطحی راستہ پیدا کرتا ہے، عالمی نقشہ کو مدنظر رکھتے ہوئے۔
    *   **لوکل پلانر:** گلوبل راستے کی پیروی کرنے کے لیے رفتار کے احکامات پیدا کرتا ہے جبکہ حقیقی وقت میں متحرک رکاوٹوں سے بچتا ہے۔
    *   **Nav2 فریم ورک:** ROS 2 نیویگیشن2 اسٹیک کو اپنانا، جو آرکیسٹریشن کے لیے بیہیویر ٹری استعمال کرتا ہے۔
3.  **لوکوموشن کنٹرول:**
    *   **ہیومینائیڈ گیٹ کنٹرولر:** دو پاؤں والے روبوٹس کے لیے اہم، یہ جزو لوکل پلانر سے رفتار کے احکامات کو مستحکم چلنے کے نمونوں میں ترجمہ کرتا ہے، توازن، پاؤں کی جگہ، اور پورے جسم کی الٹی کائینیٹکس کا انتظام کرتا ہے۔
    *   **رکاوٹ سے بچاؤ:** متحرک رکاوٹوں سے بچنے کے لیے لوکل پلانر کے ساتھ سینسر ڈیٹا کا انضمام۔
4.  **ROS 2 انٹرفیس (`navigation_node`):** ایک Nav2 پر مبنی نوڈ جو LLM پلانر سے اہداف لیتا ہے اور ہیومینائیڈ کے لوکوموشن کنٹرولر کو احکامات دیتا ہے۔

**ہارڈ ویئر کی ضروریات:** LiDAR سینسر، ڈیپتھ کیمرہ (مثلاً، Intel RealSense D435i/L515)، IMU، اور پروسیسنگ کے لیے ایک ایمبیڈڈ کمپیوٹر (NVIDIA Jetson)۔

## پرسیپشن: ماحول کو سمجھنا

مؤثر منصوبہ بندی اور عمل کے لیے، ہیومینائیڈ روبوٹ کو اپنے ارد گرد کے ماحول کو درست طریقے سے محسوس کرنے اور سمجھنے کی ضرورت ہے۔ اس میں مختلف سینسرز سے ڈیٹا پروسیس کرنا شامل ہے تاکہ اشیاء کی شناخت کی جا سکے، منظر کی جیومیٹری کو سمجھا جا سکے، اور انسانی موجودگی کو پہچانا جا سکے۔

### پرسیپشن ماڈیولز:

1.  **آبجیکٹ ڈیٹیکشن اور ریکگنیشن:**
    *   **سینسرز:** ہائی ریزولوشن RGB کیمرے۔
    *   **ماڈلز:** گہرے سیکھنے کے ماڈلز (مثلاً، YOLO, DETR) کا استعمال کرتے ہوئے بڑے ڈیٹاسیٹس (یا Isaac Sim سے مصنوعی ڈیٹا) پر تربیت یافتہ اشیاء کا پتہ لگانے اور درجہ بندی کرنے کے لیے جو انسانی احکامات میں مخصوص ہیں (مثلاً، "سرخ کپ"، "کتاب")۔
    *   **ROS 2 انٹرفیس (`object_detection_node`):** باؤنڈنگ باکسز، کلاس لیبلز، اور دریافت شدہ اشیاء کی 3D پوزیشنز کو ایک موضوع (مثلاً، `/perception/detected_objects`) پر شائع کرتا ہے۔
2.  **سین سیگمنٹیشن اور 3D ریکنسٹرکشن:**
    *   **سینسرز:** ڈیپتھ کیمرے اور/یا LiDAR۔
    *   **الگورتھم:** پوائنٹ کلاؤڈ سیگمنٹیشن یا میش ریکنسٹرکشن جیسی تکنیکوں کا استعمال کرتے ہوئے ماحول کی جیومیٹری اور معنوی ترتیب کو سمجھنا۔
    *   **درخواست:** قابل سفر سطحوں، قابل گرفت سطحوں کی شناخت، اور تصادم کو روکنا۔
3.  **ہیومن ڈیٹیکشن اور پوز ایستیمیشن:**
    *   **سینسرز:** RGB کیمرے۔
    *   **ماڈلز:** انسانی موجودگی کا پتہ لگانے اور ان کے 2D/3D پوز کا اندازہ لگانے کے لیے خصوصی گہرے سیکھنے کے ماڈلز۔
    *   **درخواست:** محفوظ انسانی-روبوٹ تعاون اور سماجی اشارے سمجھنے کے لیے اہم۔
4.  **ROS 2 انٹرفیس:** LLM پلانر کے لیے ایک مشترکہ نمائندگی میں تمام پرسیپشن آؤٹ پٹس کا انضمام۔

**ہارڈ ویئر کی ضروریات:** ہائی ریزولوشن RGB کیمرہ، ڈیپتھ کیمرہ، اور حقیقی وقت کی انفرنس کے لیے ایک قابل ایمبیڈڈ GPU۔

## ہیرا پھیری: اشیاء کے ساتھ تعامل

کیپ اسٹون پروجیکٹ کا آخری حصہ ہیومینائیڈ روبوٹ کو اپنے ماحول کے ساتھ جسمانی طور پر تعامل کرنے کے قابل بنانا ہے۔ اس کے لیے کثیر جوڑوں والے بازوؤں اور ماہر گرفتوں/ہاتھوں کا درست کنٹرول درکار ہے۔

### ہیرا پھیری پائپ لائن:

1.  **ہدف آبجیکٹ کی لوکلائزیشن:** پرسیپشن آؤٹ پٹس (آبجیکٹ ڈیٹیکشن، 3D پوز ایستیمیشن) کا استعمال کرتے ہوئے ہدف آبجیکٹ کی صحیح جگہ اور سمت کا تعین کرنا۔
2.  **الٹی کائینیٹکس (IK):** ایک مطلوبہ اینڈ-ایفیکٹر پوز (ہاتھ/گرفت کی پوزیشن اور سمت) دی گئی، IK سالور روبوٹ کے بازو کے لیے ضروری جوائنٹ زاویوں کا حساب لگاتا ہے تاکہ اس پوز تک پہنچ سکے۔
3.  **موشن پلاننگ:**
    *   **تصادم سے بچاؤ:** موشن پلانر بازو کے لیے ایک تصادم سے پاک راستہ پیدا کرتا ہے جو اس کی موجودہ ترتیب سے ہدف گرفت کی ترتیب تک جاتا ہے، خود تصادم اور ماحولیاتی رکاوٹوں سے بچتا ہے۔
    *   **رکاوٹ کی تسکین:** یقینی بناتا ہے کہ راستہ جوائنٹ کی حدود، رفتار کی حدود، اور ایکسلریشن کی حدود کا احترام کرتا ہے۔
    *   **MoveIt 2:** ROS 2 MoveIt 2 فریم ورک موشن پلاننگ اور ہیرا پھیری کے لیے ایک صنعت کا معیاری حل ہے۔
4.  **گرفت کی حکمت عملی:**
    *   **پری-گرفت پوز:** کامیاب گرفت کے لیے بہترین نقطہ نظر کے زاویے اور گرفت کے کھلنے کا تعین کرنا۔
    *   **فورس کنٹرول:** قوت کا احساس (مثلاً، گرفت میں) کو نافذ کرنا تاکہ اشیاء کو مناسب قوت کے ساتھ پکڑا جا سکے، نقصان یا پھسلنے سے بچنے کے لیے۔
    *   **گرفت کے معیار کی تشخیص:** گرفت کی کامیابی کی تصدیق کے لیے بصری یا ٹیکٹائل فیڈبیک کا استعمال۔
5.  **ROS 2 ایکشن انٹرفیس (`manipulation_action_server`):** ایک ROS 2 ایکشن سرور LLM پلانر سے ہیرا پھیری کے اہداف (مثلاً، `grasp_object(object_id, target_pose)`) وصول کرے گا، موشن پلان کو انجام دے گا، گرفت کو کنٹرول کرے گا، اور فیڈبیک اور نتائج فراہم کرے گا۔

**ہارڈ ویئر کی ضروریات:** کثیر-DOF روبوٹک بازو، ماہر گرفت/ہاتھ، فورس-ٹارک سینسرز (مضبوط گرفت کے لیے اختیاری لیکن تجویز کردہ)۔

ان پیچیدہ ماڈیولز کو کامیابی سے مربوط کر کے، کیپ اسٹون پروجیکٹ ایک مکمل فعال، ذہین ہیومینائیڈ روبوٹ کا مظاہرہ کرے گا، جو جسمانی AI کے اصولوں اور جدید روبوٹکس انجینئرنگ کا عروج دکھائے گا۔

## سسٹم آرکیٹیکچر ڈایاگرام

**(تصوری خاکہ - ایک تصویر یا تفصیلی وضاحت کے طور پر لاگو کیا جائے)**

```mermaid
graph TD
    A[انسانی آواز کا حکم] --> B[مائیکروفون ارے]
    B --> C[Whisper (آواز سے متن)]
    C --> D[ROS 2 /voice_commands/text موضوع]
    D --> E[LLM پلانر نوڈ]
    
    E -- ایکشن گولز --> F[ROS 2 ایکشن سرورز]
    F -- نیویگیشن گولز --> G[نیویگیشن اسٹیک (Nav2)]
    G --> H[ہیومینائیڈ لوکوموشن کنٹرولر]
    H --> I[ہیومینائیڈ روبوٹ (ایکچیویٹرز)]

    F -- ہیرا پھیری کے اہداف --> J[ہیرا پھیری اسٹیک (MoveIt 2)]
    J --> K[ہیومینائیڈ روبوٹ (بازو/گرفت)]
    K --> I

    L[ہیومینائیڈ روبوٹ (سینسرز)] --> M[LIDAR, ڈیپتھ کیم, RGB کیم, IMU]
    M --> N[پرسیپشن نوڈز (VSLAM, آبجیکٹ ڈیٹیکشن)]
    N --> E
    N --> G
```

**ہارڈ ویئر کی ضروریات (ہیومینائیڈ کے لیے مثال ترتیب):**

-   **ایمبیڈڈ کمپیوٹ:** NVIDIA Jetson Orin AGX (آن-روبوٹ LLM انفرنس، پرسیپشن، اور کنٹرول کے لیے)۔
-   **سینسرز:**
    *   **RGB-D کیمرہ:** Intel RealSense D435i یا L515 (گہرائی اور رنگ کی پرسیپشن کے لیے)۔
    *   **LiDAR:** RPLIDAR S1 یا اسی طرح کا (360 ڈگری ماحولیاتی میپنگ کے لیے)۔
    *   **IMU:** روبوٹ کے بیس میں مربوط یا اسٹینڈ الون (سمت اور ایکسلریشن کے لیے)۔
    *   **مائیکروفونز:** USB مائیکروفون ارے (مثلاً، ReSpeaker 4-Mic Array) یا روبوٹ کے سر میں مربوط۔
-   **ایکچیویٹرز:**
    *   **ٹارک کنٹرولڈ سروسز:** جوائنٹس کے لیے اعلیٰ درستگی، اعلیٰ ٹارک سروسز (مثلاً، Dynamixel سیریز، کسٹم ہائی پاور ایکچیویٹرز)۔
    *   **ماہر ہاتھ/گرفت:** 2-فنگر یا ملٹی-فنگر گرفت (مثلاً، Robotiq, OpenMANIPULATOR-X)۔
-   **روبوٹ پلیٹ فارم:** ایک تحقیقی ہیومینائیڈ روبوٹ پلیٹ فارم (مثلاً، Robotis OP3، کسٹم بلٹ پلیٹ فارم)۔
-   **پاور مینجمنٹ:** بیٹری پیک، پاور ڈسٹریبیوشن بورڈ۔
-   **مواصلات:** Wi-Fi ماڈیول، ایتھرنیٹ۔